{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Activation, Dense, Lambda, Input, MaxPooling2D, Dropout, Flatten, Reshape, UpSampling2D, Concatenate\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 784)).astype('float32') / 255.0\n",
    "x_test = x_test.reshape((-1, 784)).astype('float32') / 255.0\n",
    "# y_train = to_categorical(y_train, 10)\n",
    "# y_test = to_categorical(y_test, 10)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "print(mnist_digits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New training and testing data with only 0,3,1,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_pick_train = []\n",
    "indices_to_pick_test = []\n",
    "for cluster_index in [0,3,1,9]:\n",
    "    print(\"checking {}\".format(cluster_index))\n",
    "    C_i_train = np.where(y_train == cluster_index)[0].tolist()\n",
    "    indices_to_pick_train.append(C_i_train)\n",
    "    \n",
    "    C_i_test = np.where(y_test == cluster_index)[0].tolist()\n",
    "    indices_to_pick_test.append(C_i_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_pick_train = list(chain.from_iterable(indices_to_pick_train)) # flatten the 2D list\n",
    "indices_to_pick_test = list(chain.from_iterable(indices_to_pick_test)) # flatten the 2D list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(indices_to_pick_train))\n",
    "print(len(indices_to_pick_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for index in indices_to_pick_train:\n",
    "    train_data.append(x_train[index])\n",
    "    train_labels.append(y_train[index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indices_to_pick_test:\n",
    "    test_data.append(x_test[index])\n",
    "    test_labels.append(y_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_data)\n",
    "Y_train = np.array(train_labels)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_data)\n",
    "Y_test = np.array(test_labels)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_new_train_x.npy',X_train)\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_new_train_y.npy',Y_train)\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_new_test_x.npy',X_test)\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_new_test_y.npy',Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_new_train_x.npy')\n",
    "Y_train = np.load('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_new_train_y.npy')\n",
    "X_test = np.load('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_new_test_x.npy')\n",
    "Y_test = np.load('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_new_test_y.npy')\n",
    "print(X_train.shape, Y_train.shape);print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_shape = (28, 28, 1)\n",
    "original_dim = image_shape[0] * image_shape[1]\n",
    "input_shape = (original_dim,)\n",
    "batch_size = 64\n",
    "latent_dim = 5\n",
    "epochs = 30\n",
    "\n",
    "# encoder\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Reshape(image_shape)(inputs)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "#plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True)\n",
    "\n",
    "# decoder\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "#label_inputs = Input(shape=(10,), name='label')\n",
    "#x = Concatenate()([latent_inputs, label_inputs])\n",
    "x = Dense(128, activation='relu')(latent_inputs)\n",
    "x = Dense(14 * 14 * 64, activation='relu')(x)\n",
    "x = Reshape((14, 14, 64))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "outputs = Reshape(input_shape)(x)\n",
    "\n",
    "decoder = Model([latent_inputs], outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True)\n",
    "\n",
    "# variational autoencoder\n",
    "outputs = decoder([encoder(inputs)[2]])\n",
    "\n",
    "vae = Model([inputs], outputs, name='vae_mlp')\n",
    "vae.summary()\n",
    "#plot_model(vae, to_file='vae_cnn.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "#vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae_loss = reconstruction_loss + kl_loss\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "weights_file = '/home/dsarkar/compute_canada/MNIST/model/vae_cnn_mnist4_z5.h5'\n",
    "\n",
    "if os.path.exists(weights_file):\n",
    "    vae.load_weights(weights_file)\n",
    "    print('Loaded weights!')\n",
    "else:\n",
    "    history = vae.fit(X_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size, \n",
    "           validation_split = 0.2,\n",
    "            #validation_data=(X_test, Y_test),\n",
    "             verbose=1, callbacks=[es])\n",
    "    vae.save_weights('/home/dsarkar/compute_canada/MNIST/model/vae_cnn_mnist4_z5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss for latent dim = 2')\n",
    "plt.ylabel('MSE value')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_train = encoder.predict(X_train)[2]\n",
    "print(latent_space_train.shape)\n",
    "\n",
    "latent_space_test = encoder.predict(X_test)[2]\n",
    "latent_space_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_space_train = encoder.predict(X_train)[2]\n",
    "print(latent_space_train.shape)\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_train_latent_space_z5.npy',latent_space_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_test = encoder.predict(X_test)[2]\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_test_latent_space_z5.npy',latent_space_test)\n",
    "latent_space_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_train = np.load('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_train_latent_space_z5.npy')\n",
    "print(latent_space_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_space_test = np.load('/home/dsarkar/compute_canada/MNIST/dataset/MNIST4_test_latent_space_z5.npy')\n",
    "print(latent_space_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(latent_space_train)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mutual_info_score(kmeans.labels_,Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(X_train)\n",
    "normalized_mutual_info_score(kmeans.labels_,Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling on the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'C' : [0.1, 1, 10, 100]},\n",
    "                    {'kernel': ['linear'], 'C' : [0.1, 1, 10, 100]}]\n",
    "\n",
    "# ,\n",
    "#                    {'kernel': ['poly'], 'C' : [0.1, 1, 10, 100], 'gamma': [1,0.1,0.01,0.001]},\n",
    "#                    {'kernel': ['sigmoid'], 'C' : [0.1, 1, 10, 100], 'gamma': [1,0.1,0.01,0.001]}]\n",
    "score = 'accuracy'\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, scoring=score, n_jobs=-1,refit=True,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_random = []\n",
    "sd_random = []\n",
    "\n",
    "for coreset_size in range(20,501,20): # start from 20 labeled points\n",
    "    print(\"*********************** Training on {} points ***********************\".format(coreset_size))\n",
    "    accuracy = [] # calculate accuracy of 100 iterations\n",
    "    c = list(zip(X_train,Y_train))\n",
    "    iterations = 0\n",
    "    while iterations < 100: # run 100 simulations and take average \n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        for (data,label) in random.sample(c,coreset_size):\n",
    "            train_data.append(data)\n",
    "            train_labels.append(label)  \n",
    "        train_x = np.array(train_data)\n",
    "        train_y = np.array(train_labels)\n",
    "        \n",
    "        print()\n",
    "        print(\"Distribution of data in the training points\")\n",
    "        print(Counter(train_y)) \n",
    "\n",
    "        clf.fit(train_x, train_y)\n",
    "        print(\"Best parameters set found on {} data points:\".format(coreset_size))\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        y_true, y_pred = Y_test, clf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_true, y_pred))\n",
    "        iterations += 1\n",
    "    accuracy = np.asarray(accuracy)\n",
    "    mean_accuracy = accuracy.mean()\n",
    "    sd_accuracy = accuracy.std()\n",
    "    \n",
    "    mean_random.append(mean_accuracy)\n",
    "    sd_random.append(sd_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_random = np.array(mean_random)\n",
    "sd_random = np.array(sd_random)\n",
    "\n",
    "\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/result/mean_accuracy_rs_MNIST4_input_space.npy',mean_random)\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/result/sd_accuracy_rs_MNIST4_input_space.npy',sd_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_random = []\n",
    "sd_random = []\n",
    "\n",
    "for coreset_size in range(20,501,20): # start from 20 labeled points\n",
    "    print(\"*********************** Training on {} points ***********************\".format(coreset_size))\n",
    "    accuracy = [] # calculate accuracy of 100 iterations\n",
    "    c = list(zip(latent_space_train,Y_train))\n",
    "    iterations = 0\n",
    "    while iterations < 100: # run 100 simulations and take average \n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        for (data,label) in random.sample(c,coreset_size):\n",
    "            train_data.append(data)\n",
    "            train_labels.append(label)  \n",
    "        train_x = np.array(train_data)\n",
    "        train_y = np.array(train_labels)\n",
    "        \n",
    "        print()\n",
    "        print(\"Distribution of data in the training points\")\n",
    "        print(Counter(train_y)) \n",
    "\n",
    "        clf.fit(train_x, train_y)\n",
    "        print(\"Best parameters set found on {} data points:\".format(coreset_size))\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        y_true, y_pred = Y_test, clf.predict(latent_space_test)\n",
    "        accuracy.append(accuracy_score(y_true, y_pred))\n",
    "        iterations += 1\n",
    "    accuracy = np.asarray(accuracy)\n",
    "    mean_accuracy = accuracy.mean()\n",
    "    sd_accuracy = accuracy.std()\n",
    "    \n",
    "    mean_random.append(mean_accuracy)\n",
    "    sd_random.append(sd_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_random = np.array(mean_random)\n",
    "sd_random = np.array(sd_random)\n",
    "\n",
    "\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/result/mean_accuracy_rs_MNIST4_z5.npy',mean_random)\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/result/sd_accuracy_rs_MNIST4_z5.npy',sd_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means coreset on latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_coreset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=40, random_state=0).fit(latent_space_train)\n",
    "np.unique(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_coreset = []\n",
    "sd_accuracy_coreset = []\n",
    "\n",
    "for coreset_size in range(40,501,40): # start from 60 labeled points\n",
    "    print(\"*********************** Training on {} points ***********************\".format(coreset_size))\n",
    "\n",
    "    accuracy = []\n",
    "    m = int(coreset_size/40) # m=B/K, number of points from each cluster\n",
    "    iterations = 0\n",
    "    while iterations < 100: # run 100 simulations and take average \n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        indices_to_pick = []\n",
    "        \n",
    "        print(\"Choosing {} points from each cluster\".format(m))\n",
    "        for cluster_index in range(40):\n",
    "            C_i = np.where(kmeans.labels_ == cluster_index)[0].tolist()\n",
    "            sample_i = random.sample(C_i, m)\n",
    "            indices_to_pick.append(sample_i)\n",
    "        \n",
    "        indices_to_pick = list(chain.from_iterable(indices_to_pick)) # flatten the 2D list\n",
    "        \n",
    "        assert len(indices_to_pick)==coreset_size, \"Sample size mismatch!!!!\"\n",
    "        \n",
    "        for index in indices_to_pick:\n",
    "            train_data.append(latent_space_train[index])\n",
    "            train_labels.append(Y_train[index]) \n",
    "        \n",
    "        train_x = np.array(train_data)\n",
    "        train_y = np.array(train_labels)\n",
    "        \n",
    "        print()\n",
    "        print(\"Distribution of data in the training points\")\n",
    "        print(Counter(train_y))\n",
    "\n",
    "        clf.fit(train_x, train_y)\n",
    "        print(\"Best parameters set found on {} data points:\".format(coreset_size))\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        y_true, y_pred = Y_test, clf.predict(latent_space_test)\n",
    "        accuracy.append(accuracy_score(y_true, y_pred))\n",
    "        iterations += 1\n",
    "\n",
    "    accuracy = np.asarray(accuracy)\n",
    "    mean_accuracy = accuracy.mean()\n",
    "    sd_accuracy = accuracy.std()\n",
    "\n",
    "\n",
    "    mean_accuracy_coreset.append(mean_accuracy)\n",
    "    sd_accuracy_coreset.append(sd_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_coreset = np.array(mean_accuracy_coreset)\n",
    "sd_accuracy_coreset = np.array(sd_accuracy_coreset)\n",
    "\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/result/mean_accuracy_cs_MNIST4_z5_K40.npy',mean_accuracy_coreset)\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/result/sd_accuracy_cs_MNIST4_z5_K40.npy',sd_accuracy_coreset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_coreset #after 100 iterations each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # K-means coreset on input space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=40, random_state=0).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_coreset = []\n",
    "sd_accuracy_coreset = []\n",
    "\n",
    "for coreset_size in range(40,501,40): # start from 60 labeled points\n",
    "    print(\"*********************** Training on {} points ***********************\".format(coreset_size))\n",
    "\n",
    "    accuracy = []\n",
    "    m = int(coreset_size/40) # m=B/K, number of points from each cluster\n",
    "    iterations = 0\n",
    "    while iterations < 100: # run 100 simulations and take average \n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        indices_to_pick = []\n",
    "        \n",
    "        print(\"Choosing {} points from each cluster\".format(m))\n",
    "        for cluster_index in range(40):\n",
    "            C_i = np.where(kmeans.labels_ == cluster_index)[0].tolist()\n",
    "            sample_i = random.sample(C_i, m)\n",
    "            indices_to_pick.append(sample_i)\n",
    "        \n",
    "        indices_to_pick = list(chain.from_iterable(indices_to_pick)) # flatten the 2D list\n",
    "        \n",
    "        assert len(indices_to_pick)==coreset_size, \"Sample size mismatch!!!!\"\n",
    "        \n",
    "        for index in indices_to_pick:\n",
    "            train_data.append(X_train[index])\n",
    "            train_labels.append(Y_train[index]) \n",
    "        \n",
    "        train_x = np.array(train_data)\n",
    "        train_y = np.array(train_labels)\n",
    "        \n",
    "        print()\n",
    "        print(\"Distribution of data in the training points\")\n",
    "        print(Counter(train_y))\n",
    "\n",
    "        clf.fit(train_x, train_y)\n",
    "        print(\"Best parameters set found on {} data points:\".format(coreset_size))\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        y_true, y_pred = Y_test, clf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_true, y_pred))\n",
    "        iterations += 1\n",
    "\n",
    "    accuracy = np.asarray(accuracy)\n",
    "    mean_accuracy = accuracy.mean()\n",
    "    sd_accuracy = accuracy.std()\n",
    "\n",
    "\n",
    "    mean_accuracy_coreset.append(mean_accuracy)\n",
    "    sd_accuracy_coreset.append(sd_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_coreset = np.array(mean_accuracy_coreset)\n",
    "sd_accuracy_coreset = np.array(sd_accuracy_coreset)\n",
    "\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/result/mean_accuracy_cs_MNIST4_input_space_K40.npy',mean_accuracy_coreset)\n",
    "np.save('/home/dsarkar/compute_canada/MNIST/result/sd_accuracy_cs_MNIST4_input_space_K40.npy',sd_accuracy_coreset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
